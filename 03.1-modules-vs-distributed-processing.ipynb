{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modular vs. Distributed Processing\n",
    "\n",
    "The discovery of the Fusiform Face Area (FFA; Kanwisher, McDermott, and Chun, 1997) was a landmark discovery. This was a region that processed, not simple line orientations or gabor patches, but actual faces. The mean activity signal for face stimuli was larger than the mean for houses and objects. Thus, the FFA was preferential to faces, and by extension, it was inferred that all other cognitive processing must also be localized in brain regions yet to be discovered. \n",
    "\n",
    "The local nature of processing was challenged by another study (Haxby et al., 2001). Instead of looking at the mean activity of a set of voxels, this study examined the _pattern of activity_ of a set of voxels. Thus, if the mean activity was similar for two conditions, but the pattern of activity across a set of voxels was different across the two conditions, we can discriminate between the two conditions. Using this technique, it was shown that faces are not represented just in the FFA alone, but are distributed across a variety of brain regions. This led to the distributed view of face processing.\n",
    "\n",
    "In this notebook, you will perform a decoding analysis in the FFA and the parahippocampal place area (PPA) using the VDC dataset. To recap, the FFA was shown as a face processing region and the PPA as a scene processing region. Specifically, you will analyze the patterns of activity in these ROIs in the following ways: \n",
    "\n",
    ">1. Can we discriminate scenes vs. objects in the FFA?  \n",
    "      The FFA was shown to be a preferred region for face processing. If we can decode scenes vs. objects in this region, it implies that there is discriminable information for these two categories in the FFA. Thus, the FFA does not just represent faces, but scenes and objects too. Also scenes are not exclusively represented in the PPA. \n",
    "           \n",
    ">2. Can we discriminate faces vs. objects in the PPA?  \n",
    "      The PPA was shown to be a preferred region for scene processing. If we can decode faces vs. objects in this region, it implies that there is discriminable information for these two categories in the PPA. Thus, faces are not only represented in the FFA but in the PPA too. Also, the PPA does not just represent scenes, but faces and objects too.\n",
    "      \n",
    "\n",
    "\n",
    "## Goal of this script:\n",
    "    1. Replicate the analysis that led to the modular vs. distributed processing debate.\n",
    "    \n",
    "### Pre-requisites:\n",
    "Data loading, normalization, and classification.\n",
    "\n",
    "Terms to be familiar with: FFA, PPA, n-way classification. \n",
    "\n",
    "## Table of Contents\n",
    "**1. Load Data**\n",
    " \n",
    "\n",
    "[Modular vs Distributed Processing](#mod_dist)\n",
    "> [2.1 FFA](#mod_dist_ffa)  \n",
    "> [2.2 PPA](#mod_dist_ppa) \n",
    "\n",
    "### Exercises\n",
    ">[Exercise 1](#ex1)  [2](#ex2)  [3](#ex3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "%matplotlib inline \n",
    "%autosave 5\n",
    "sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some helper functions\n",
    "from utils import load_vdc_stim_labels, load_vdc_masked_data\n",
    "from utils import shift_timing, reshape_data\n",
    "# load some constants\n",
    "from utils import vdc_data_dir, vdc_all_ROIs, vdc_label_dict, vdc_n_runs, vdc_hrf_lag, vdc_TR, vdc_TRs_run\n",
    "\n",
    "print('Here\\'re some constants, which is specific for VDC data:')\n",
    "print('data dir = %s' % (vdc_data_dir))\n",
    "print('ROIs = %s' % (vdc_all_ROIs))\n",
    "print('Labels = %s' % (vdc_label_dict))\n",
    "print('number of runs = %s' % (vdc_n_runs))\n",
    "print('1 TR = %.2f sec' % (vdc_TR))\n",
    "print('HRF lag = %.2f sec' % (vdc_hrf_lag))\n",
    "print('num TRs per run = %d' % (vdc_TRs_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data \n",
    "\n",
    "Load the data for the FFA and PPA masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TR\n",
    "def label2TR(stim_label, num_runs, TR, TRs_run):\n",
    "\n",
    "    # Calculate the number of events/run\n",
    "    _, events = stim_label.shape\n",
    "    events_run = int(events / num_runs)    \n",
    "    \n",
    "    # Preset the array with zeros\n",
    "    stim_label_TR = np.zeros((TRs_run * 3, 1))\n",
    "\n",
    "    # Cycle through the runs\n",
    "    for run in range(0, num_runs):\n",
    "\n",
    "        # Cycle through each element in a run\n",
    "        for i in range(events_run):\n",
    "\n",
    "            # What element in the concatenated timing file are we accessing\n",
    "            time_idx = run * (events_run) + i\n",
    "\n",
    "            # What is the time stamp\n",
    "            time = stim_label[2, time_idx]\n",
    "\n",
    "            # What TR does this timepoint refer to?\n",
    "            TR_idx = int(time / TR) + (run * (TRs_run - 1))\n",
    "\n",
    "            # Add the condition label to this timepoint\n",
    "            stim_label_TR[TR_idx]=stim_label[0, time_idx]\n",
    "        \n",
    "    return stim_label_TR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a subject\n",
    "sub = 'sub-01';\n",
    "\n",
    "# Convert the shift from secs to TRs\n",
    "shift_size = int(vdc_hrf_lag / vdc_TR) \n",
    "\n",
    "# Load subject labels\n",
    "stim_label_allruns = load_vdc_stim_labels(sub) \n",
    "\n",
    "# Load the fMRI data\n",
    "epi_mask_data_all = load_vdc_masked_data(vdc_data_dir, sub, vdc_all_ROIs)\n",
    "\n",
    "# Convert the timing into TR indexes\n",
    "TRs_run = int(epi_mask_data_all[0].shape[1] / vdc_n_runs)\n",
    "stim_label_TR = label2TR(stim_label_allruns, vdc_n_runs, vdc_TR, TRs_run)\n",
    "\n",
    "# Shift the data some amount\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)\n",
    "\n",
    "# Select and reshape FFA data \n",
    "bold_data_FFA, labels = reshape_data(\n",
    "    stim_label_TR_shifted, epi_mask_data_all[vdc_all_ROIs.index('FFA')])\n",
    "\n",
    "# Select and reshape PPA data \n",
    "bold_data_PPA, _ = reshape_data(\n",
    "    stim_label_TR_shifted, epi_mask_data_all[vdc_all_ROIs.index('PPA')])\n",
    "\n",
    "# What is the dimensionality of the data? We need the first dim to be the same\n",
    "print('FFA: ', bold_data_FFA.shape)\n",
    "print('PPA: ', bold_data_PPA.shape)\n",
    "print('labels: ', labels.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the classifiers that will be used\n",
    "svc = LinearSVC()\n",
    "\n",
    "# load run ids (works similarity to cv_ids)\n",
    "run_ids = stim_label_allruns[5,:] - 1 \n",
    "\n",
    "def normalize(bold_data_, run_ids):\n",
    "    \"\"\"normalized the data within each run\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    bold_data_: np.array, n_stimuli x n_voxels\n",
    "    run_ids: np.array or a list\n",
    "    \n",
    "    Return\n",
    "    --------------\n",
    "    normalized_data\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data = []\n",
    "    for r in range(vdc_n_runs):\n",
    "        data.append(scaler.fit_transform(bold_data_[run_ids == r, :]))\n",
    "    normalized_data = np.vstack(data)\n",
    "    return normalized_data\n",
    "    \n",
    "\"\"\"\n",
    "copy your `decode` function from the previous notebook (03) in place of the function below\n",
    "\"\"\"\n",
    "def decode(X, y, cv_ids, model): \n",
    "    pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modular vs. distributed processing<a id=\"mod_dist\"></a>\n",
    "\n",
    "Perform a sequence of analysis that will help inform you on the modular vs distributed processing debate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Modular vs. distributed processing: FFA <a id=\"mod_dist_ffa\"></a>\n",
    "\n",
    "**Exercise 1:**<a id=\"ex1\"></a> Decode Objects vs. Scenes from FFA. \n",
    "\n",
    "What do you infer about the processing of faces, objects, and scenes in the FFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Modular vs. distributed processing: PPA  <a id=\"mod_dist_ffa\"></a>\n",
    "\n",
    "**Exercise 2:**<a id=\"ex2\"></a> Decode Objects vs. Faces from PPA. \n",
    "\n",
    "What do you infer about the processing of faces, objects, and scenes in the PPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:**<a id=\"ex4\"></a> Consolidating all your inferences what are your views on modular vs. distributed processing in the brain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your answer here.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
