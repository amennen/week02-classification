{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Classifiers \n",
    "**V.0.2 - Beta, [contributions](#contributions)**\n",
    "\n",
    "The spam folder did not exist on email systems in the recent past. Emails that were relevant to the reader had to be manually (and painfully) sorted from an array of emails soliciting money or selling hoax products, among other things. The classification of our emails, by machines, into relevant emails and spam, has advanced to such a degree that we now take for granted that all the spam email we receive is automatically routed to the spam folder, needing little oversight from us. These machine classifiers use algorithms that are applicable to a wide variety of fields: written language; spoken language (e.g. \"Hello Google\", \"Alexa\", \"Siri\"); navigating driverless cars; and we'll also use them to understand brain activity. \n",
    "\n",
    " This notebook, will walk you through the steps of labeling the fMRI signal, and then training and testing classifiers.\n",
    "\n",
    "\n",
    "## Goal of this script\n",
    "Using this script you will be able to use a classifier on your dataset. Additionally, you will be familiarized with the use of functions in your code. Specifically, we will accomplish the following:  \n",
    ">1. Assign labels to every time-point (TR) in the dataset\n",
    ">2. Time-shift the signal to be classified, taking into consideration the delayed hemodynamic response\n",
    ">3. Collect BOLD data for all runs into one array\n",
    ">4. Test out a classifier (SVM) on a group of subjects with a fixed set of parameters.\n",
    ">5. Compute basic statistics to estimate means and confidence intervals  \n",
    "\n",
    "### Pre-requisite\n",
    "You should be familiar with the data loading and normalization steps in the 02-data-handling notebook.\n",
    "\n",
    "Important terms you should be familiar with: TRs, labels.\n",
    "\n",
    "## Table of Contents\n",
    "[1. Loading and Normalization](#load_data)  \n",
    ">[1.1 Label fMRI data](#label_data)  \n",
    ">[1.2 Plot the different conditions](#plot_boxcar)  \n",
    ">[1.3 Hemodynamic Lag: Time shift the labels](#label_shift)  \n",
    ">[1.4 Load the fMRI data](#load_fmri)  \n",
    "\n",
    "[2. Classification](#classification)  \n",
    ">[2.1 Reshape data](#reshape)  \n",
    ">[2.2 Model training](#model_training)  \n",
    ">[2.3 Model testing](#model_testing)  \n",
    ">[2.4 Test across participants](#across_ppts)  \n",
    "\n",
    "Exercises\n",
    "> [1](#ex1) [2](#ex2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset** For this script we will use a localizer dataset from [Kim et al. (2017)](https://doi.org/10.1523/JNEUROSCI.3272-16.2017) again. Just to recap: The localizer consisted of 3 runs with 5 blocks of each category (faces, scenes and objects) per run. Each block was presented for 15s. Within a block, a stimulus was presented every 1.5s (1 TR). Between blocks, there was 15s (10 TRs) of fixation. Each run was 310 TRs. In the matlab stimulus file, the first row codes for the stimulus category for each trial (1 = Faces, 2 = Scenes, 3 = Objects). The 3rd row contains the time (in seconds, relative to the start of the run) when the stimulus was presented for each trial.\n",
    "\n",
    "By this stage of analysis, you should have completed motion correction and any other required pre-processing on the fMRI data. In this example, all the data have been pre-processed for you.\n",
    "\n",
    "**Python Indexing Reminder:** Everything begins at [0], not [1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys \n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "import os \n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from nilearn.input_data import NiftiMasker\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from brainiak.utils.fmrisim import _double_gamma_hrf as hrf_func\n",
    "from brainiak.utils import fmrisim as sim\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "%autosave 5\n",
    "sns.set(style = 'white', context='poster', rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some helper functions\n",
    "from utils import load_vdc_mask, load_vdc_epi_data, load_vdc_masked_data, label2TR\n",
    "# load some constants\n",
    "from utils import vdc_data_dir, vdc_all_ROIs, vdc_label_dict, vdc_n_runs, vdc_hrf_lag, vdc_TR, vdc_TRs_run\n",
    "\n",
    "print('Here\\'re some constants, which is specific for VDC data:')\n",
    "print('data dir = %s' % (vdc_data_dir))\n",
    "print('ROIs = %s' % (vdc_all_ROIs))\n",
    "print('Labels = %s' % (vdc_label_dict))\n",
    "print('number of runs = %s' % (vdc_n_runs))\n",
    "print('1 TR = %.2f sec' % (vdc_TR))\n",
    "print('HRF lag = %.2f sec' % (vdc_hrf_lag))\n",
    "print('num TRs per run = %d' % (vdc_TRs_run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong> Note on data file paths:</strong> The data directory points to a specific location on the Adroit cluster, for the NEU480 class, at Princeton University, <strong> and no change to the path is required </strong>. If you are not in the NEU480 class or running on another cluster (e.g., scotty or spock) you will need to make a change to the data file path variable `vdc_data_dir` in `utils.py`.  \n",
    "<br>\n",
    "\n",
    "<strong>If you are not in the Princeton or Yale class</strong>, that uses this dataset, check the brainiak tutorial repository for information on where to download this dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory structure of the VDC dataset\n",
    "!head -50 {vdc_data_dir}README.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Normalization<a id=\"load_data\"></a>\n",
    "\n",
    "As per the procedure used in the notebook, 02-data-handling, load in the vdc dataset. To load in the data we are going to first run a series of commands that set the variable name and then store the data. The timing data are stored as a matrix by concatenating all runs.\n",
    "\n",
    "Last time, you used a function called `load_vdc_stim_labels(subject_name)`, which import the labels for the fMRI data. The cell below shows source code for that function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variables\n",
    "sub = 'sub-01';\n",
    "\n",
    "# Preset the variable size\n",
    "stim_label = [];\n",
    "stim_label_allruns = [];\n",
    "for run in range(1, vdc_n_runs+1):\n",
    "    # Specify the input variables\n",
    "    in_file = '%s%s/ses-day2/design_matrix/%s_localizer_0%d.mat' % (vdc_data_dir, sub, sub, run)\n",
    "    # Load in data from matlab\n",
    "    stim_label = scipy.io.loadmat(in_file);\n",
    "    stim_label = np.array(stim_label['data']);\n",
    "\n",
    "    # Store the data\n",
    "    if run == 1:\n",
    "        stim_label_allruns = stim_label;\n",
    "    else:       \n",
    "        stim_label_allruns = np.hstack((stim_label_allruns, stim_label))\n",
    "\n",
    "print(\"Loaded labels for\", sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong> Usage of hstack and vstack:</strong> hstack stacks arrays in sequence horizontally (column wise) and vstack stacks arrays in sequence vertically (row wise). More details can be found at \n",
    "[np.hstack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html) and [np.vstack](https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong> Creating and using functions is an efficient way to program. </strong>  \n",
    "\n",
    "Although we can load the data with the set of commands above, this isn't an efficient way to code if we want to re-use this code. A better way to code is to make a **function** that performs the loading of the data and then can be called with different input variables to load in new data. We do this below.\n",
    "\n",
    "Functions are extremely useful for many reasons and so should be used everywhere: they allow you to remove redundancy in your code, reduce the likelihood of an error since if you update the function you update all of its uses, and you make your code much more readable.\n",
    "\n",
    "A useful tutorial on functions is found [here](https://www.datacamp.com/community/tutorials/functions-python-tutorial).\n",
    "\n",
    "One important thing to be aware of is how variables are shared between your workspace and a function. If you have variables in your workspace (i.e., any variables you have created in the usage of jupyter) then they will usually be accessible/usable in a function, regardless of whether they are used as input parameters. However, any variables you create in a function cannot be used in your workspace if you don't return them as outputs. For this reason it is easier if you keep the names of the variables in your function separate from the names in your workspace. This is turtles all the way down: if you have a function within a function then variables will be shared in the same way.\n",
    "\n",
    "The function that we will now create is called `load_vdc_stim_labels`. You can see that this is a function since it starts with `def`.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function for loading in the labels\n",
    "def load_vdc_stim_labels(vdc_data_dir, subject_id):\n",
    "    stim_label = [];\n",
    "    stim_label_concatenated = [];\n",
    "    for run in range(1, vdc_n_runs+1):\n",
    "        in_file = '%s%s/ses-day2/design_matrix/%s_localizer_0%d.mat' % (\n",
    "            vdc_data_dir, subject_id, subject_id, run)\n",
    "        # Load in data from matlab\n",
    "        stim_label = scipy.io.loadmat(in_file);\n",
    "        stim_label = np.array(stim_label['data']);\n",
    "\n",
    "        # Store the data\n",
    "        if run == 1:\n",
    "            stim_label_concatenated = stim_label;\n",
    "        else:       \n",
    "            stim_label_concatenated = np.hstack((stim_label_concatenated, stim_label))\n",
    "    print(\"Loaded labels for\", subject_id)\n",
    "    return stim_label_concatenated\n",
    "\n",
    "# Note: you won't see the printed \"Loaded labels for...\" until you run the function with inputs (see next step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to call this function and get its outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the labels of the data\n",
    "stim_label_allruns = load_vdc_stim_labels(vdc_data_dir, sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Label fMRI data <a id=\"label_data\"></a>\n",
    "\n",
    "We need category labels to train a classifier for every timepoint in the fMRI data. Below, we assign these labels to every timepoint of fMRI data collected. These timepoints are also referred to as TRs.\n",
    "\n",
    "This subject has 310 TRs but all others have 311. Some TRs will not have a label since there was no stimulus presented (e.g., the fixation periods between blocks). \n",
    "\n",
    "Recall that the third row of the timing file is the start time of a trial in seconds since the start of the run. We can thus convert each time stamp to a specific TR by taking that time stamp and dividing by the TR duration (1.5 s). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Preset variables\n",
    "_, events = stim_label_allruns.shape\n",
    "events_run = int(events / vdc_n_runs)\n",
    "\n",
    "# Preset the array with zeros\n",
    "stim_label_TR = np.zeros((vdc_TRs_run * 3, 1))\n",
    "\n",
    "# Cycle through the runs\n",
    "for run in range(vdc_n_runs):\n",
    "\n",
    "    # Cycle through each element in a run\n",
    "    for i in range(events_run):\n",
    "\n",
    "        # What element in the concatenated timing file are we accessing\n",
    "        time_idx = run * (events_run) + i\n",
    "\n",
    "        # What is the time stamp\n",
    "        time = stim_label_allruns[2, time_idx]\n",
    "\n",
    "        # What TR does this timepoint refer to?\n",
    "        TR_idx = int(time / vdc_TR) + (run * (vdc_TRs_run - 1))\n",
    "\n",
    "        # Add the condition label to this timepoint\n",
    "        stim_label_TR[TR_idx] = stim_label_allruns[0, time_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function `label2TR` that will return a list of category label values for each TR.  It uses the following variables: `stim_labels`, `num_runs`, `TR`, and `TRs_run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function\n",
    "stim_label_TR = label2TR(stim_label_allruns, vdc_n_runs, vdc_TR, vdc_TRs_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Plot the different conditions <a id=\"plot_boxcar\"></a>\n",
    "\n",
    "Below we create a box car plot (looks like square waves) of the timing of different conditions for the first run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a sequence of timepoints that a TR occurred on\n",
    "tr_time = np.arange(0, (vdc_TRs_run - 1) * 1.5 + 1, 1.5)\n",
    "\n",
    "# Plot the data\n",
    "f, ax = plt.subplots(1,1, figsize = (12,5))\n",
    "ax.plot(tr_time, stim_label_TR[0:vdc_TRs_run, 0], c='orange')\n",
    "\n",
    "ax.set_ylabel('Stimuli labels')\n",
    "ax.set_xlabel('TR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Hemodynamic Lag: Time shift the labels <a id=\"label_shift\"></a>\n",
    "\n",
    "The brain response that we measure with fMRI is slow which means there is a lag between when events occur and when we expect to find changes in the BOLD signal. Below we plot the expected neural response to a single event.\n",
    "More details about hemodynamic response can be found [here].(https://en.wikipedia.org/wiki/Haemodynamic_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help on hrf_func\n",
    "help(hrf_func)\n",
    "\n",
    "hrf = hrf_func(temporal_resolution=1)\n",
    "\n",
    "# Plot the canonical double gamma HRF\n",
    "f, ax = plt.subplots(1,1, figsize = (10, 5))\n",
    "ax.plot(range(30), hrf)\n",
    "\n",
    "ax.set_title(\"Hemodynamic Response Function (HRF)\")\n",
    "ax.set_xlabel('Time in secs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1 Modelling the HRF**  \n",
    "\n",
    "We now model the HRF for two different scenarios: a sequence of stimuli presented close together (block design), and individual stimuli presented far apart (event design). \n",
    "\n",
    "**1.3.1.a. What does the expected neural response look like if events are shown continuously for 10 seconds?**\n",
    "\n",
    "The various parameters used are described below: \n",
    "> `trDuration`:how long is each TR.  \n",
    "> `numTRs`: total number of TRs in the experiment.  \n",
    "> `onsets`: timepoint in experiment when the stimulus is turned on.  \n",
    ">`event_duration`: how long is each event. This could be the duration of block (for block design experiments) or a trial (for event-related designs).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the volume parameters\n",
    "trDuration = 2  # seconds\n",
    "numTRs = 40 # How many TRs will you generate?\n",
    "total_time = int(numTRs * trDuration)\n",
    "temporal_res = 0.5 #1/trDuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The event onsets at 20 seconds.\n",
    "# The block lasts for 10 seonds (5 TRs).\n",
    "\n",
    "stim_A = sim.generate_stimfunction(onsets=[20], \n",
    "                                                   event_durations=[10], \n",
    "                                                   total_time=total_time,\n",
    "                                                   temporal_resolution=temporal_res \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stim_A)\n",
    "plt.title('Stimulus Timing: Block 10sec (5 TRs)')\n",
    "plt.xlabel('TRs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_func_A = sim.convolve_hrf(stimfunction=stim_A,\n",
    "                                   tr_duration=trDuration,\n",
    "                                   temporal_resolution=temporal_res,\n",
    "                                   scale_function=1,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_func_A)\n",
    "plt.title('HRF: Block 10sec (5 TRs)')\n",
    "plt.xlabel('TRs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.1.b. Next we examine the expected neural response look like if two events evoke the same response, 10 seconds apart?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The events onset at 20 and 30 seconds.\n",
    "# Each event lasts for 2 seconds (1 TR).\n",
    "stim_B = sim.generate_stimfunction(onsets=[20,30], \n",
    "                                                   event_durations=[2,2], \n",
    "                                                   total_time=total_time,\n",
    "                                                   temporal_resolution=temporal_res \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stim_B)\n",
    "plt.title('Stimulus Timing: Two events 10sec(5 TRs) apart')\n",
    "plt.xlabel('TRs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_func_B = sim.convolve_hrf(stimfunction=stim_B,\n",
    "                                   tr_duration=2,\n",
    "                                   temporal_resolution=temporal_res,\n",
    "                                   scale_function=1,\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(signal_func_B)\n",
    "plt.title('HRF: Two events 10sec (5 TRs) apart')\n",
    "plt.xlabel('TRs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**  \n",
    "Note the difference in the plots for the block and the two event signals. When stimuli are presented in blocks, the HRF sustains itself for the duration of the block. For the two event stimuli, presented far apart, the hemodynamic signal increases and falls after the first event and then reovers after the second stimuli is shown, before finally going down to baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** <a id=\"ex1\"></a>From what you just learned about the HRF, why might an experimenter want to keep stimuli separated in time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3.2 Time Shift the VDC Labels**  \n",
    "To account for the hemodynamic lag in the neural response, we can shift the timecourse of events. First let's plot this timecourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_conditions = len(vdc_label_dict)\n",
    "cur_pals = sns.color_palette('colorblind', n_colors=n_conditions)\n",
    "\n",
    "# Create a sequence of timepoints that a TR occurred on\n",
    "tr_time = np.arange(0, (vdc_TRs_run - 1) * 1.5 + 1, 1.5)\n",
    "time_vals = stim_label_allruns[2, 0:150]\n",
    "labels = stim_label_allruns[0, 0:150]\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize = (14, 5))\n",
    "    \n",
    "# plot the label for each condition\n",
    "for i_cond in range(n_conditions): \n",
    "    label = list(vdc_label_dict.keys())[i_cond]\n",
    "    temp_mask = label == labels\n",
    "    ax.scatter(time_vals[temp_mask], labels[temp_mask], \n",
    "               color = cur_pals[i_cond], marker = 'o')\n",
    "ax.legend(vdc_label_dict.values())\n",
    "    \n",
    "# plot the stimuli as a line \n",
    "# ax.plot(time_vals, labels, color = 'black', alpha = .5)\n",
    "ax.plot(tr_time, stim_label_TR[0:vdc_TRs_run, 0], c='orange', alpha = .5)\n",
    "\n",
    "ax.set_yticks(list(vdc_label_dict.keys()))\n",
    "ax.set_yticklabels(vdc_label_dict.values())\n",
    "\n",
    "ax.set_title('Stimulus Presentation for Run 1')\n",
    "ax.set_xlabel('Time (seconds)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to incorporate this time-shift when we extract the BOLD signal for classification. One way to accomplish this is to shift the labels by 4-5 seconds and extract the BOLD signal for the non-zero labels. As one TR = 1.5 seconds, we'll shift by 3 TRs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Shift the data a certain amount\n",
    "print(vdc_hrf_lag) # In seconds what is the lag between a stimulus onset and the peak bold response\n",
    "shift_size = int(vdc_hrf_lag / vdc_TR)  # Convert the shift into TRs\n",
    "\n",
    "# Create a function to shift the size\n",
    "def shift_timing(label_TR, TR_shift_size):\n",
    "    \n",
    "    # Create a short vector of extra zeros\n",
    "    zero_shift = np.zeros((TR_shift_size, 1))\n",
    "\n",
    "    # Zero pad the column from the top\n",
    "    label_TR_shifted = np.vstack((zero_shift, label_TR))\n",
    "\n",
    "    # Don't include the last rows that have been shifted out of the time line\n",
    "    label_TR_shifted = label_TR_shifted[0:label_TR.shape[0],0]\n",
    "    \n",
    "    return label_TR_shifted\n",
    "\n",
    "# Apply the function\n",
    "stim_label_TR_shifted = shift_timing(stim_label_TR, shift_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the boxcar to the plot\n",
    "\n",
    "f, ax = plt.subplots(1,1, figsize = (12,5))\n",
    "ax.plot(tr_time, stim_label_TR_shifted[0:310], c='orange')\n",
    "\n",
    "ax.set_ylabel('Stimuli labels')\n",
    "ax.set_xlabel('TR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the shifted boxcar is not plotted automatically into the cell above. This is because plotting goes into the figure after plt.figure() was called and will keep writing there (automatically sets 'hold on' if you are familiar with matlab) until you call a new plt.figure()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Load the fMRI data<a id=\"load_fmri\"></a>\n",
    "\n",
    "As in the exercise from week 1 we will load in, mask, and z-score the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('available ROIs: ', vdc_all_ROIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_name = 'FFA'\n",
    "\n",
    "# Apply the function to pull out the mask data\n",
    "epi_mask_data_all = load_vdc_masked_data(vdc_data_dir, sub, vdc_all_ROIs)\n",
    "\n",
    "# Check the dimensionality of the data\n",
    "print('voxel by TR matrix - shape: ', epi_mask_data_all[vdc_all_ROIs.index(roi_name)].shape)\n",
    "print('label list - shape: ', stim_label_TR_shifted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classification<a id=\"classification\"></a>\n",
    "\n",
    "We will now build a very basic classifier to categorize our data. \n",
    "\n",
    "In the example of email, spam mail has certain characteristics (or features) such as: sent from an unknown sender; sells products; asks for money deposits. These features are converted into mathematical vectors in a language feature space and computer programs are trained on these feature vectors,  _from known examples of relevant and spam emails_, to categorize emails as either relevant or spam. For brain activity measured by fMRI, the voxel signals serve as the features. If we were showing pictures of faces, scenes, and objects, and collecting fMRI data while the subject was viewing the pictures, we know what picture was shown at each time-point. The stimulus type at each timepoint serves as the stimulus label. The signal from voxels at each timepoint correspond to the features of that picture. From this known set of stimulus labels and features, we can train a classifier to distinguish between pictures of faces, scenes, and objects.\n",
    "\n",
    "Once the classifier training is accomplished, we still do not know how this classifier will perform when it has to make a prediction on fMRI signals for an image that it was not trained on, e.g. a new picture of a face. To determine if the classifier can predict the stimulus label of an _unseen stimulus_, we test the classifier we created, on an _unseen dataset_, and determine its prediction accuracy. If the classifier makes predictions above chance (random guessing will lead to a 33.33% accuracy for 3 categories), the classifier has \"mind read\" the category of the stimulus.\n",
    "\n",
    "We will try the SVM classifier in this exercise.  \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Each classifier has a number of parameters that can be changed to affect the sensitivity of the classification (called hyper-parameters). For now, we will hard code these parameters. We will cover more principled ways to do classification in a future notebook.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Reshape data <a id=\"reshape\"></a>\n",
    "\n",
    "First, we extract the time points for which we have stimulus labels (only Faces, Places, and Objects)-- i.e., we drop the time-points from the BOLD signal that refer to the fixation periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract bold data for non-zero labels\n",
    "def reshape_data(label_TR_shifted, masked_data_all):\n",
    "    label_index = np.nonzero(label_TR_shifted)\n",
    "    label_index = np.squeeze(label_index)\n",
    "    # Pull out the indexes\n",
    "    indexed_data = np.transpose(masked_data_all[:,label_index])\n",
    "    nonzero_labels = label_TR_shifted[label_index] \n",
    "    return indexed_data, nonzero_labels\n",
    "\n",
    "bold_data, labels = reshape_data(\n",
    "    stim_label_TR_shifted, epi_mask_data_all[vdc_all_ROIs.index(roi_name)]\n",
    ")\n",
    "\n",
    "# What is the dimensionality of the data? We need the first dim to be the same\n",
    "print(bold_data.shape)\n",
    "print(labels.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Leave-One-Run-Out (LORO) training and testing  <a id=\"model_training\"></a>\n",
    "\n",
    "We have our vdc dataset to train our classifier. From where do we get an unseen dataset to test the accuracy of the classifier? We use LORO. A typical fMRI experiment has multiple runs, wherein the same categories are shown in all runs. One trick we can use is to create an unseen dataset is to not train the classifier on all runs, let's say we Leave One Run Out from the training set. This left out run is now our unseen dataset -- the classifier was never trained on this data. We can use this dataset to test the accuracy of the classifier.\n",
    "\n",
    "Furthermore, we can loop through the runs, leaving out a run each time (also known as folding), and test multiple times. Let's say we have 3 runs: 1,2,3. Then, we can establish the sequence of training and testing, giving us 3 independent tests of the classifier accuracy, as shown in the table below.\n",
    "\n",
    "| Fold | Train | Test |\n",
    "| --- | --- | --- |\n",
    "|1 |  run1, run2 | run3 |\n",
    "|2 |  run2, run3 | run1 |\n",
    "|3 |  run3, run1 | run2 |\n",
    "  \n",
    "\n",
    "**Using LORO ensures that the classifier is tested on an unseen, independent dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Manually create a left-out run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get run ids (works similarity to cv_ids)\n",
    "run_ids = stim_label_allruns[5,:] - 1 \n",
    "\n",
    "# select a run\n",
    "holdout_run_ids = 0\n",
    "# make an index list with one run left out.\n",
    "train_runs = run_ids != holdout_run_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at what the runs that will be used for training (value=1) and runs that \n",
    "# will be used for testing (value=0).\n",
    "plt.plot(train_runs)\n",
    "plt.title('train_runs')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.2 Normalization**\n",
    "\n",
    "This is an important step in classification (see notebook 02-data-handling). Normalization puts feature values in a similar range of values. This reduces variance across features and prevents classifiers from being biased towards features with large values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(bold_data_, run_ids):\n",
    "    \"\"\"normalized the data within each run\n",
    "    \n",
    "    Parameters\n",
    "    --------------\n",
    "    bold_data_: np.array, n_stimuli x n_voxels\n",
    "    run_ids: np.array or a list\n",
    "    \n",
    "    Return\n",
    "    --------------\n",
    "    normalized_data\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    data = []\n",
    "    for r in range(vdc_n_runs):\n",
    "        data.append(scaler.fit_transform(bold_data_[run_ids == r, :]))\n",
    "    normalized_data = np.vstack(data)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bold_data_normalized = normalize(bold_data, run_ids)\n",
    "\n",
    "# split the training set and test set ... \n",
    "X_train = bold_data_normalized[train_runs,]\n",
    "y_train = labels[train_runs]\n",
    "X_test = bold_data_normalized[np.logical_not(train_runs),]\n",
    "y_test = labels[np.logical_not(train_runs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Classifiers\n",
    "\n",
    "We can use a variety of classifiers for our analysis. They are easily accessed by calling scikit-learn libraries. In the example below, we use a linear support vector machine. It is one of the most commonly used classifiers in cognitive neuroscience as it is quite to robust to low number of training samples and outliers in the data.\n",
    "\n",
    "You can also explore a variety of classifiers here: http://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1. Build a classifier and test on the held out run.\n",
    "\n",
    "The `X_train` data was created by excluding one run. We use this to train a classifier and then test it on `X_test`, the dataset that was created from the held out run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a classifier model on the training set \n",
    "model = LinearSVC(C=1)\n",
    "# is hardcoded. Optimizing these parameters will be covered in later notebooks.\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# compute your evaluation on the test set\n",
    "score = model.score(X_test, y_test)\n",
    "print('Accuracy = %s' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Build a classifier by training and testing on multiple folds.\n",
    "\n",
    "We use scikit learn libraries to automate the task of creating multiple folds. The `PredefinedSplit` method does this automatically.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<strong> The test dataset must always be independent of the train dataset.</strong>\n",
    "<br>\n",
    "No matter which method is adopted, you should always make sure that the train and test datasets are independent. It's best to exercise caution and make sure any folding method that you use is working the way you want it to. You could do this be getting the rowids of the training and test datasets and making sure that they have no overlap and belong to unique runs.\n",
    "<div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, if you do not separate the training and test dataset, you will get inflated accuracies. We will cover more of this aspect in future notebooks.\n",
    "\n",
    "** This is what you should not be doing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"double dipping\"\"\"\n",
    "\n",
    "# data normalizer \n",
    "scaler = StandardScaler()\n",
    "# classifier \n",
    "model = LinearSVC() \n",
    "\n",
    "X_train = scaler.fit_transform(bold_data)\n",
    "# fit a svm\n",
    "model.fit(X_train, labels)\n",
    "# calculate the accuracy for the hold out run\n",
    "score = model.score(X_train, labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now we split the dataset into training and testing in the correct way.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### handle the train-test split with sklearn\n",
    "Here's the documentation of `PredefinedSplit`: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"demo\"\"\"\n",
    "demo_run_ids = [1, 1, 0, 0]\n",
    "ps = PredefinedSplit(demo_run_ids)\n",
    "for train_index, test_index in ps.split():\n",
    "    print(\"train ids:\", train_index, \"test ids:\", test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all runs \n",
    "scores = []\n",
    "\n",
    "# split data according to run ids \n",
    "ps = PredefinedSplit(run_ids)\n",
    "\n",
    "# classifier \n",
    "model = LinearSVC() \n",
    "for train_index, test_index in ps.split():\n",
    "    X_train, X_test = bold_data_normalized[train_index], bold_data_normalized[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    # fit a svm\n",
    "    model.fit(X_train, y_train)\n",
    "    # calculate the accuracy for the hold out run\n",
    "    score = model.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.3 Classifier Performance**  \n",
    "To determine how good is the accuracy of a classifier, we can compare it to random guessing. In the VDC dataset, we have 3 equally represnted sets of stimuli, and thus random guessing will result in being correct 1/3 of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Perform the analysis across participants <a id=\"across_ppts\"></a>\n",
    "\n",
    "The next step is to run a classifier on a group of subjects. We will now create the condition variable, stim_label_TR, for all subjects and then use it to pull out the relevant participant data and then feed it into a classifier.\n",
    "\n",
    "Before you run this command, make sure you save your notebook. You will be loading in a lot of data and so it might run into memory issues and crash your job. If you do have issues, change how many cores/memory you are using when you launch this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loops through subjects and calls a classifier. Use this function for the question below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pack the classification code as a function \n",
    "def decode(X, y, cv_ids, model): \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    X: np.array, n_stimuli x n_voxels\n",
    "    y: np.array, n_stimuli, \n",
    "    cv_ids: np.array - n_stimuli, \n",
    "    \n",
    "    Return\n",
    "    --------------\n",
    "    models: a list of models \n",
    "    scores: a list of scores\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    models = []\n",
    "    ps = PredefinedSplit(cv_ids)\n",
    "    for train_index, test_index in ps.split():\n",
    "        # enter your code below... \n",
    "        \n",
    "        # split the data  \n",
    "        \n",
    "        # train the decoder\n",
    "        \n",
    "        # score it on a hold-out set\n",
    "    \n",
    "        # save stuff \n",
    "        models.append(model)\n",
    "        scores.append(score)\n",
    "    return models, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the function \n",
    "model = LinearSVC()  \n",
    "models, scores = decode(X=bold_data_normalized, y=labels, cv_ids=run_ids, model=model)\n",
    "print('Decoding accuracy across the 3 runs: ', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** <a id=\"ex2\"></a> What is the average accuracy across all participants of SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions <a id=\"contributions\"></a> \n",
    "\n",
    "M. Kumar, C. Ellis and N. Turk-Browne produced the initial notebook  \n",
    "T. Meissner minor edits  \n",
    "Q. Lu a lot of edits... "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
